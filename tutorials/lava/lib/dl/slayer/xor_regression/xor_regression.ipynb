{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XOR: A minimalistic regression tutorial for LAVA-DL\n",
        "by [Alexander Henkes](https://orcid.org/0000-0003-4615-9271)\n",
        "---\n",
        "\n",
        "In this tutorial we want to solve a simple regression task using [spiking neural\n",
        "networks](https://en.wikipedia.org/wiki/Spiking_neural_network) (SNNs) and the [**LAVA-DL** library](https://github.com/lava-nc/lava-dl). The presented approach tries to stay as\n",
        "minimalistic as possible, though it is easy to expand it to much more complex\n",
        "problems. A basic understanding of spiking neural networks is asumed. If you start from zero, take a look at [this tutorials](https://snntorch.readthedocs.io/en/latest/tutorials/index.html) first.\n"
      ],
      "metadata": {
        "id": "msLwzgSMpGqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and random seeds"
      ],
      "metadata": {
        "id": "wmEmgbWivHT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Solve XOR with LIFs using LAVA/SLAYER.\"\"\"\n",
        "import lava.lib.dl.slayer as slayer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "SEED = 666\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbo4p4EKtlxt",
        "outputId": "388b2d96-e347-492a-e6bb-720b9b7dbb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79d0d4241610>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) it the task of relating some ($n_x$-dimensional, real-valued) input to some ($n_y$-dimensional, real-valued) output, such that\n",
        "\n",
        "\\begin{align}\n",
        "    f: \\mathbb{R}^{n_x} &\\to \\mathbb{R}^{n_y} \\\\\n",
        "    \\mathbf{x} &\\mapsto \\mathbf{y}.\n",
        "\\end{align}\n",
        "\n",
        "This simple statement can describe all kinds of nonlinear functions whith possibly very complicated behavior. If we do not know an analytical expression for our complicated function $f$ but have access to some input data $\\mathbf{x}$ and output data $\\mathbf{y}$, we can approximate it using a neural network $\\mathcal{N}$, such that\n",
        "\n",
        "\\begin{equation}\n",
        "    f \\approx \\mathcal{N}.\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "mtaGSyb7puyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR\n",
        "Here, we take a look at the [XOR problem](https://en.wikipedia.org/wiki/Exclusive_or), which relates a two-dimensional input pair to a one-dimensional output\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{XOR}: \\mathbb{R} \\times \\mathbb{R} &\\to \\mathbb{R} \\\\\n",
        "    (A, B) &\\mapsto A \\oplus B\n",
        "\\end{align}\n",
        "\n",
        "It can be described by the following table:\n",
        "\n",
        "\\begin{array}{c}\n",
        "A & B & A \\oplus B \\\\ \\hline\n",
        "0 & 0 & 0 \\\\\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "1 & 1 & 0 \\\\ \\hline\n",
        "\\end{array}\n",
        "\n",
        "We see that our dataset is indeed quite minimalistic, it merely consists of four samples. We can define the dataset using the [standard PyTorch approach](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files). SNNs are inherently time-dependent, but the XOR dataset is static. We simply generate a pseudo-time axis by repeating every sample for as many time steps as we like."
      ],
      "metadata": {
        "id": "VZkX-6tBucs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XOR(torch.utils.data.Dataset):\n",
        "    \"\"\"XOR dataset.\n",
        "\n",
        "    Produce a torch.dataset for the XOR problem. It consists of two inputs\n",
        "    and one output correspoinding to the following logic table:\n",
        "\n",
        "    Input   |   Output\n",
        "    ==================\n",
        "    (0, 0)  |   (0)\n",
        "    (0, 1)  |   (1)\n",
        "    (1, 0)  |   (1)\n",
        "    (1, 1)  |   (0)\n",
        "    ==================\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, time):\n",
        "        \"\"\"Initialize dataset.\n",
        "\n",
        "        The dataset consists of two-dimensional input features and\n",
        "        one-dimensional output labels. The axis convention\n",
        "\n",
        "            (BATCH, TIME, FEATURE)\n",
        "\n",
        "        is used. The parameter 'time' controlls the number of\n",
        "        discrete pseudo-time steps.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        time : int\n",
        "            Number of discrete time steps needed for LIF-type neurons.\n",
        "\n",
        "        \"\"\"\n",
        "        self.feature = torch.Tensor(\n",
        "            [\n",
        "                [0, 0],\n",
        "                [0, 1],\n",
        "                [1, 0],\n",
        "                [1, 1],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.label = torch.Tensor(\n",
        "            [\n",
        "                [0],\n",
        "                [1],\n",
        "                [1],\n",
        "                [0],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.feature = torch.unsqueeze(self.feature, -1)\n",
        "        self.label = torch.unsqueeze(self.label, -1)\n",
        "\n",
        "        self.feature = torch.repeat_interleave(\n",
        "            input=self.feature, repeats=time, dim=-1\n",
        "        )\n",
        "        self.label = torch.repeat_interleave(\n",
        "            input=self.label, repeats=time, dim=-1\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return length of dataset.\n",
        "\n",
        "        The length of the dataset is defined as the length of the\n",
        "        first axis, the batch axis.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            Number of unique samples in the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        return len(self.feature[:, 0, 0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return a single sample from the dataset.\n",
        "\n",
        "        Return a single sample from the dataset using the index variable 'idx'.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Index of the sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Sample 'idx' from the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.feature[idx, :, :], self.label[idx, :, :]\n"
      ],
      "metadata": {
        "id": "nCDtNSJ4tvaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network\n",
        "\n",
        "In this tutorial we want to use SNNs to fit real-valued XOR problem. You can solve this as a classification task, probably using spikes directly, but we chose a different approach which translates well to all kinds of regression scenarios.\n",
        "\n",
        "The problem in regression with SNNs lies in the binary (or unary, if you like) nature of information travel between neurons. On the one hand, this leads to temporal- and inter-spike sparsity and therefore, to massive energy savings on neuromorphic hardware, on the other hand representing real-valued functions with spikes is not straightforward.\n",
        "\n",
        "First, we need to convert our real-valued input to spikes using some sort of encoder. In this tutorial, we will use a simple CUBA, a second-order variant of the classical [LIF](https://neuronaldynamics.epfl.ch/online/Ch1.S3.html) with richer neural dynamics. The encoder simply adds the input values constantly over time to a CUBA neuron, which generates spikes\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{encoder}: \\mathbb{R} \\times \\mathbb{R} &\\to \\{0, 1\\}^t.\n",
        "\\end{align}\n",
        "\n",
        "In **LAVA-DL**, this is realized by the `slayer.block.cuba.Input` layer, which organises several neurons in a stack. In between we can use as many spiking layers as we want\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{spiking neuron}: \\{0, 1\\}^t &\\to \\{0, 1\\}^t.\n",
        "\\end{align}\n",
        "\n",
        "Here, we chose the `slayer.block.cuba.Dense` layer which combines a dense feed-forward neural network with CUBA dynamics. For the output we take the membrane potential of the neuron in the last layers. For more details for this approach you can take a look at [https://arxiv.org/abs/2210.03515](https://arxiv.org/abs/2210.03515). The potential is a real-valued number and can be extracted via the `slayer.block.cuba.Affine` layer. It acts as some sort of decoder, from spikes to real-valued numbers\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{decoder}: \\{0, 1\\}^t &\\to \\mathbb{R}.\n",
        "\\end{align}\n",
        "\n",
        "Again, we can define the network using [PyTorch](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html). For details on the nasty details, see the [**LAVA-DL** documentation](https://lava-nc.org/lava-lib-dl/index.html)."
      ],
      "metadata": {
        "id": "8onwPO-WvSvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(torch.nn.Module):\n",
        "    \"\"\"LIF network.\n",
        "\n",
        "    A network consisting of the following topology:\n",
        "\n",
        "    Layer\n",
        "    ===============\n",
        "    - BlockCubaInput\n",
        "    - BlockCubaDense\n",
        "    - BlockCubaDense\n",
        "    - BlockCubaAffine\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize network.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        cuba_params = {\n",
        "            \"threshold\": 0.1,\n",
        "            \"current_decay\": 0.9,\n",
        "            \"voltage_decay\": 0.9,\n",
        "            \"tau_grad\": 1,\n",
        "            \"scale_grad\": 1,\n",
        "            \"scale\": 1 << 6,\n",
        "            \"norm\": None,\n",
        "            \"dropout\": None,\n",
        "            \"shared_param\": True,\n",
        "            \"persistent_state\": False,\n",
        "            \"requires_grad\": False,\n",
        "            \"graded_spike\": False,\n",
        "        }\n",
        "\n",
        "        width = 32\n",
        "\n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [\n",
        "                slayer.block.cuba.Input(\n",
        "                    neuron_params=cuba_params, count_log=False\n",
        "                ),\n",
        "                slayer.block.cuba.Dense(\n",
        "                    neuron_params=cuba_params,\n",
        "                    in_neurons=2,\n",
        "                    out_neurons=width,\n",
        "                    count_log=False,\n",
        "                ),\n",
        "                slayer.block.cuba.Dense(\n",
        "                    neuron_params=cuba_params,\n",
        "                    in_neurons=width,\n",
        "                    out_neurons=width,\n",
        "                    count_log=False,\n",
        "                ),\n",
        "                slayer.block.cuba.Affine(\n",
        "                    neuron_params=cuba_params,\n",
        "                    in_neurons=width,\n",
        "                    out_neurons=1,\n",
        "                    dynamics=False,\n",
        "                    count_log=False,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        count = []\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "            count.append(torch.mean(x).item())\n",
        "\n",
        "        return x, torch.as_tensor(count)\n"
      ],
      "metadata": {
        "id": "dTFsXv7rt05a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop\n",
        "\n",
        "The training using [**LAVA-DL**](https://lava-nc.org/dl.html#lava-dl-workflow) is carried out like a PyTorch training loop, but some details are handled via the library directly. First, we chose an [optimizer](https://pytorch.org/docs/stable/optim.html) and define a [dataloader](https://pytorch.org/docs/stable/optim.html). The training itself including logging is carried out by `slayer.utils.Assistant()` in conjunction with `slayer.utils.LearningStats()`. With the help of a `lambda` function we define a simple mean-squared error loss on the last time step of our pseudo-time (remember: SNNs are inherently time-dependent, therefore we introduced a pseudo-time in our static data to be able to make use of the neuron dynamics). Finally, we loop over our training set. Additionally, we track the number of spikes produced by every layer. This gives us information about the level of sparsity of our network."
      ],
      "metadata": {
        "id": "IBjlXUFIvWD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, dataset, epochs):\n",
        "    \"\"\"Train the network.\"\"\"\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=1e-3)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset, batch_size=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    stats = slayer.utils.LearningStats(\n",
        "        loss_str=\"loss\",\n",
        "        loss_unit=\"\",\n",
        "        accuracy_str=\"acc\",\n",
        "        accuracy_unit=\"\",\n",
        "    )\n",
        "\n",
        "    assistant = slayer.utils.Assistant(\n",
        "        net=net,\n",
        "        error=lambda output, target: torch.nn.functional.mse_loss(\n",
        "            output[:, :, -1].flatten(), target[:, :, -1].flatten()\n",
        "        ),\n",
        "        optimizer=optimizer,\n",
        "        stats=stats,\n",
        "        classifier=None,\n",
        "        count_log=True,\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (feature, label) in enumerate(dataloader):\n",
        "            _, count = assistant.train(feature, label)\n",
        "\n",
        "            prediction = net(dataset.feature)[0]\n",
        "            prediction = torch.round(prediction)\n",
        "            prediction = torch.abs(prediction)\n",
        "\n",
        "            correct = torch.sum(prediction[:, :, -1] == label[:, :, -1])\n",
        "            stats.training.correct_samples = correct.data.item()\n",
        "\n",
        "            print(f\"\\r[Epoch {epoch:3d}/{epochs}] {stats}\", end=\"\")\n",
        "\n",
        "        stats.update()\n",
        "\n",
        "    return stats, count\n"
      ],
      "metadata": {
        "id": "5YGkNWVkt6Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function\n",
        "\n",
        "In the main function we go through the following steps:\n",
        "\n",
        "\n",
        "1.   Create the SNN `net`\n",
        "2.   Create the XOR-dataset `dataset` with pseudo-time `time`\n",
        "3.   Train the network using `train()`\n",
        "4.   Print sparsity information `spike_activity`\n",
        "5.   Predict `prediction` and print some results!\n",
        "\n"
      ],
      "metadata": {
        "id": "T1T9Zi4CvYn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Execute main function.\"\"\"\n",
        "    net = Network()\n",
        "    dataset = XOR(time=10)\n",
        "\n",
        "    _, count = train(net=net, dataset=dataset, epochs=500)\n",
        "\n",
        "    spike_activity = [str(round(i.item() * 100, 2)) for i in count.numpy()]\n",
        "    spike_activity = \"| \" + \"\".join(x + \"% | \" for x in spike_activity)\n",
        "    print(f\"\\n\\nSpike activity per layer: {spike_activity}\\n\")\n",
        "\n",
        "    prediction = net(dataset.feature)[0]\n",
        "    prediction = torch.round(prediction)\n",
        "    prediction = torch.abs(prediction)\n",
        "\n",
        "    print(\n",
        "        f\"{'Input:':<12}{dataset.feature[:, :, -1].detach().numpy().tolist()}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"{'Output:':<12}{dataset.label[:, :, -1].detach().numpy().tolist()}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"{'Prediction:':<12}{prediction[:, :, -1].detach().numpy().tolist()}\"\n",
        "    )\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "vukYGg5RtO8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run it!\n",
        "\n",
        "You can run everything (again and again) using the following cell. The layout of the notebook was chosen in order to get a nice `.py` script when exporting. It can aid as a solid basis for your own experiments!"
      ],
      "metadata": {
        "id": "4pFzUkHeva37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6hdAZTkt96z",
        "outputId": "623cb951-567f-4188-e860-73a16b155f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 499/500] Train loss =     0.00122 (min =     0.00049)     acc = 1.00000 (max = 1.00000) \n",
            "\n",
            "Spike activity per layer: | 45.0% | 17.81% | 15.78% | 34.06% | \n",
            "\n",
            "Input:      [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
            "Output:     [[0.0], [1.0], [1.0], [0.0]]\n",
            "Prediction: [[0.0], [1.0], [1.0], [0.0]]\n"
          ]
        }
      ]
    }
  ]
}