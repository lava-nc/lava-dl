{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoyer Training Tutorial\n",
    "\n",
    "This tutorial begins by exploring the cuba_hoyer neuron and its associated Hoywe spike function, providing a foundational understanding of their functionality. \n",
    "\n",
    "Following this introduction, we will delve into enabling Hoyer training, which encompasses the utilization of Hoyer Neurons, relevant layers, and Hoyer Regularization techniques. \n",
    "\n",
    "For illustrative purposes, we will employ the VGG16 network architecture and the CIFAR10 dataset as our primary examples throughout this guide."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The structure of Hoyer Neuron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 init function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lava.lib.dl.slayer.neuron import neuron_params, Neuron\n",
    "class HoyerNeuron(Neuron):\n",
    "    def __init__(\n",
    "        self, threshold, current_decay, voltage_decay,\n",
    "        tau_grad=1, scale_grad=1, scale=1 << 6, norm=None, dropout=None,\n",
    "        shared_param=True, persistent_state=False, requires_grad=False,\n",
    "        graded_spike=False, num_features=1, T=1, hoyer_type='sum', momentum=0.9, delay=False\n",
    "    ):\n",
    "        super(HoyerNeuron, self).__init__(...)\n",
    "        \n",
    "        self.learnable_thr = nn.Parameter(torch.FloatTensor([self.threshold]), requires_grad=True)\n",
    "        self.T = T\n",
    "        self.hoyer_type = hoyer_type\n",
    "        self.num_features = num_features\n",
    "        self.momentum = 0.9\n",
    "        if self.num_features > 1:\n",
    "            self.bn = nn.BatchNorm2d(num_features=self.num_features)\n",
    "        self.delay = delay\n",
    "\n",
    "        if self.num_features > 1: \n",
    "            if self.hoyer_type == 'sum':\n",
    "                self.register_buffer('running_hoyer_ext', torch.zeros([1, 1, 1, 1, T]))\n",
    "            else:\n",
    "                self.register_buffer('running_hoyer_ext', torch.zeros([1, self.num_features, 1, 1, T]))\n",
    "        else:\n",
    "            self.register_buffer('running_hoyer_ext', torch.zeros([1, 1, T]))\n",
    "\n",
    "        self.clamp()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hoyer neuron extends the functionality of the Cuba neuron. In addition to inheriting the initialization properties of the Cuba neuron, the Hoyer neuron incorporates a trainable threshold to enhance performance. When connected to a Convolutional (Conv) layer, it also integrates a Batch Normalization (BN) layer. Furthermore, the Hoyer neuron requires the time step and the number of input features to determine the dimensions of the `running_hoyer_ext`, which also depends on the specified Hoyer type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Spike function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def spike(self, voltage, hoyer_ext=1.0):\n",
    "        spike = HoyerSpike.apply(\n",
    "            voltage,\n",
    "            hoyer_ext,\n",
    "            self.tau_rho * TAU_RHO_MULT,\n",
    "            self.scale_rho * SCALE_RHO_MULT,\n",
    "            self.graded_spike,\n",
    "            self.voltage_state,\n",
    "            # self.s_scale,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        if self.persistent_state is True:\n",
    "            with torch.no_grad():\n",
    "                self.voltage_state = leaky_integrator.persistent_state(\n",
    "                    self.voltage_state, spike[..., -1]\n",
    "                ).detach().clone()\n",
    "\n",
    "        if self.drop is not None:\n",
    "            spike = self.drop(spike)\n",
    "\n",
    "        return spike\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spike function is almost the same as the original one except we apply the `HoyerSpike` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculate Hoyer loss $$ \\frac{(\\sum |x|)^2}{\\sum x^2 }$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def cal_hoyer_loss(self, x, thr=None):\n",
    "        if thr:\n",
    "            x[x>thr] = thr\n",
    "        x[x<0.0] = 0.0\n",
    "        # avoid division by zero\n",
    "        return (torch.sum(torch.abs(x))**2) / (torch.sum(x**2) + 1e-9)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Forward function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def forward(self, input):\n",
    "        if self.num_features > 1 and hasattr(self, 'bn'):\n",
    "            B,C,H,W,T = input.shape\n",
    "            input = self.bn(input.permute(4,0,1,2,3).reshape(T*B,C, H, W).contiguous())\\\n",
    "                .reshape(T,B,C,H,W).permute(1,2,3,4,0).contiguous()\n",
    "        _, voltage = self.dynamics(input)\n",
    "        self.hoyer_loss = self.cal_hoyer_loss(torch.clamp(voltage.clone(), min=0.0, max=1.0), 1.0)\n",
    "        voltage = voltage / self.learnable_thr\n",
    "        if self.training:\n",
    "            clamped_input = torch.clamp(voltage.clone().detach(), min=0.0, max=1.0)\n",
    "            dim = tuple(range(clamped_input.ndim-1))\n",
    "            if self.hoyer_type == 'sum':\n",
    "                hoyer_ext = torch.sum(clamped_input**2, dim=dim) / \n",
    "                (torch.sum(torch.abs(clamped_input), dim=dim))\n",
    "            else:\n",
    "                hoyer_ext = torch.sum((clamped_input)**2, dim=(0,2,3), keepdim=True) / \n",
    "                torch.sum(torch.abs(clamped_input), dim=(0,2,3), keepdim=True)\n",
    "\n",
    "            hoyer_ext = torch.nan_to_num(hoyer_ext, nan=1.0)\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                if self.delay:\n",
    "                    # delay hoyer ext\n",
    "                    self.running_hoyer_ext[..., 0] = 0\n",
    "                    self.running_hoyer_ext = torch.roll(self.running_hoyer_ext, shifts=-1, dims=-1)\n",
    "                    self.running_hoyer_ext = self.momentum * hoyer_ext + (1 - self.momentum) * self.running_hoyer_ext\n",
    "                else:\n",
    "                    # do not delay hoyer ext\n",
    "                    self.running_hoyer_ext = self.momentum * hoyer_ext + (1 - self.momentum) * self.running_hoyer_ext\n",
    "                \n",
    "        else:\n",
    "            hoyer_ext = self.running_hoyer_ext\n",
    "        output = self.spike(voltage, hoyer_ext)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward pass, if the preceding synapse is a Conv layer, a Batch Normalization (BN) layer is applied first. Then, the dynamics function calculates the voltage, which is also used to calculate the Hoyer loss. The voltage is normalized by a trainable threshold, diverging from the fixed value of 1. Subsequently, running_hoyer_ext is updated akin to BN behavior. Finally, spikes are generated from the voltage using the spike function.\n",
    "\n",
    "$$ Ext_{hoyer} = \\frac{\\sum x^2 }{\\sum |x|} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The structure of Hoyer Spike Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def _hoyer_spike_backward(\n",
    "    voltage, threshold, tau_rho, scale_rho,\n",
    "    graded_spike=False\n",
    "):\n",
    "    grad_inp = torch.zeros_like(voltage).cuda()\n",
    "\n",
    "    grad_inp[voltage > 0] = 1.0\n",
    "    grad_inp[voltage > 2.0] = 0.0\n",
    "\n",
    "    return 0.5 * grad_inp\n",
    "\n",
    "\n",
    "class HoyerSpike(Spike):\n",
    "    derivative = None\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_spikes):\n",
    "        voltage, threshold, tau_rho, scale_rho, graded_spike \\\n",
    "            = ctx.saved_tensors\n",
    "        graded_spike = True if graded_spike > 0.5 else False\n",
    "        return (\n",
    "            _hoyer_spike_backward(\n",
    "                voltage, threshold, tau_rho, scale_rho, graded_spike\n",
    "            ) * grad_spikes,\n",
    "            None, None, None, None, None, None\n",
    "        )\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward part, we compare the normalized voltage with the Hoyer Ext to decide whether to emit spikes. For the backward part, the surrogate function is shown in the figure. (The scale is 0.5 in the code.)\n",
    "\n",
    "<img src=\"figs/hoyer_spike_bp.png\" width=\"300\" height=\"200\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import copy\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Dataset\n",
    "\n",
    "Since CIFAR10 is a common static dataset, we use the same approach to process the data as we did with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels      = 10\n",
    "batch_size  = 128\n",
    "normalize   = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "transform_train = transforms.Compose([\n",
    "                            transforms.RandomCrop(32, padding=4), # this line can improve 2%\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.ToTensor(),\n",
    "                            normalize])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "train_dataset   = datasets.CIFAR10(root='./cifar_data', train=True, download=True,transform =transform_train)\n",
    "test_dataset    = datasets.CIFAR10(root='./cifar_data', train=False, download=True, transform=transform_test)\n",
    "train_loader    = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "test_loader     = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=8, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define Network Structure\n",
    "\n",
    "All the Hoyer version of cuba layers are stored in lava.lib.dl.slayer.block.cuba_hoyer. Currently, we implement ```cuba_hoyer.Conv```, ```cuba_hoyer.Dense```, ```cuba_hoyer.Affine```, ```cuba_hoyer.Pool```.\n",
    "\n",
    "In addition to the original parameters in Cuba layers, ```cuba_hoyer.Conv``` has three extra parameters: \n",
    "- T: the whole time step of the input data;\n",
    "- hoyer_type: 'sum' means all channels share one threshold, 'cw' means every channel has its own threshold;\n",
    "- num_features: the number of features/channels (used for channel-wise threshold).\n",
    "\n",
    "The hoyer_type of ```cuba_hoyer.Dense``` and ```cuba_hoyer.Affine``` must be 'sum' and the num_features must be set to 1. ```cuba_hoyer.Pool``` does not need the 3 extra parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lava.lib.dl.slayer.block.cuba_hoyer as cuba_hoyer\n",
    "\n",
    "cfg = {\n",
    "    'VGG5': [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512],\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n",
    "}\n",
    "\n",
    "class VGG_Lava(nn.Module):\n",
    "    def __init__(self, vgg_name='VGG16', labels=10, dataset = 'CIFAR10', time_steps=1):\n",
    "        super(VGG_Lava, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.T = time_steps\n",
    "        sdnn_params = { \n",
    "                'threshold'     : 1,    # delta unit threshold\n",
    "                'current_decay' : 1,    # u[t] = (1 - alpha_u) * u[t-1] + x[t]\n",
    "                'voltage_decay' : 1,    # v[t] = (1 - alpha_v) * v[t-1] + u[t] + bias\n",
    "                'tau_grad'      : 1.0,  # delta unit surrogate gradient relaxation parameter\n",
    "                'scale_grad'    : 1.0,  # delta unit surrogate gradient scale parameter\n",
    "                'requires_grad' : True, # trainable threshold\n",
    "            }\n",
    "        sdnn_cnn_params = { \n",
    "                **sdnn_params,                            # copy all sdnn_params\n",
    "                'dropout' : slayer.neuron.Dropout(p=0.1), # neuron dropout\n",
    "                'T': self.T,                              # time steps   \n",
    "                'hoyer_type': 'cw',\n",
    "            }\n",
    "        sdnn_dense_params = { \n",
    "                **sdnn_params,                            # copy all sdnn_cnn_params\n",
    "                'dropout' : slayer.neuron.Dropout(p=0.1), # neuron dropout\n",
    "                'T': self.T,                              # time steps   \n",
    "                'num_features': 1, # set 1 for linear layer to avoid bn and set different shape of hoyer ext\n",
    "            }\n",
    "        self.weight_norm = False\n",
    "        self.delay = True\n",
    "        self.delay_shift = True\n",
    "        self.cfg = cfg[vgg_name]\n",
    "        self.sdnn_cnn_params = sdnn_cnn_params\n",
    "        self.features = self._make_layers(cfg[vgg_name], sdnn_cnn_params, sdnn_params)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            slayer.block.cuba.Flatten(),\n",
    "            cuba_hoyer.Dense(sdnn_dense_params, 2048, 4096, weight_norm=self.weight_norm, delay=self.delay, delay_shift=self.delay_shift),\n",
    "            cuba_hoyer.Dense(sdnn_dense_params, 4096, 4096, weight_norm=self.weight_norm, delay=self.delay, delay_shift=self.delay_shift),\n",
    "            cuba_hoyer.Affine(sdnn_params, 4096, labels, weight_norm=self.weight_norm),\n",
    "        )\n",
    "        self.blocks = torch.nn.Sequential(*(list(self.features)+list(self.classifier)))\n",
    "        del self.features\n",
    "        del self.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        count = []\n",
    "        event_cost = torch.zeros(1).to(x.device)\n",
    "        if self.dataset == 'CIFAR10':\n",
    "            x = slayer.utils.time.replicate(x, self.T)\n",
    "        elif self.dataset == 'CIFAR10DVS':\n",
    "            x = x.permute(1, 2, 3, 4, 0).contiguous()\n",
    "\n",
    "        for i,block in enumerate(self.blocks): \n",
    "            x = block(x)\n",
    "            if hasattr(block, 'neuron'):\n",
    "                count.append(torch.sum((torch.abs(x[..., 1:]) > 0).to(x.dtype)).item())\n",
    "        out = x[:,:,-1]\n",
    "        return out, event_cost, torch.FloatTensor(count).reshape((1, -1)).to(x.device)\n",
    "\n",
    "    def _make_layers(self, cfg, sdnn_cnn_params, sdnn_params):\n",
    "        layers = []\n",
    "        in_channels = 3 if self.dataset == 'CIFAR10' else 2\n",
    "        if self.dataset == 'IMAGENET':\n",
    "            cfg.append('M')\n",
    "        for i,x in enumerate(cfg):\n",
    "            if x == 'M':\n",
    "                continue\n",
    "            sdnn_cnn_params['num_features'] = x\n",
    "            conv = cuba_hoyer.Conv(sdnn_cnn_params,  in_channels, x, 3, padding=1, stride=1, weight_scale=1, weight_norm=self.weight_norm, delay=self.delay, delay_shift=self.delay_shift)\n",
    "            if i+1 < len(cfg) and cfg[i+1] == 'M':\n",
    "                layers += [conv,\n",
    "                        cuba_hoyer.Pool(sdnn_params, 2, stride=2, weight_scale=1, weight_norm=False, delay=self.delay, delay_shift=self.delay_shift)]\n",
    "            else:\n",
    "                layers += [conv]\n",
    "            in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def merge_bn(self):\n",
    "        # merge batch normalization layer to convolutional layer\n",
    "        new_layers = list(self.blocks)\n",
    "        for i, b in enumerate(new_layers):\n",
    "            if isinstance(b, slayer.block.cuba.Flatten):\n",
    "                self.blocks = torch.nn.Sequential(*new_layers)  \n",
    "                return\n",
    "            if hasattr(b, 'neuron'):\n",
    "                if hasattr(b.neuron, 'bn'):\n",
    "                    gamma = b.neuron.bn.weight\n",
    "                    beta = b.neuron.bn.bias\n",
    "                    mean = b.neuron.bn.running_mean\n",
    "                    var = b.neuron.bn.running_var\n",
    "                    eps = b.neuron.bn.eps\n",
    "                    print('merge bn layer: ', i)\n",
    "                    W = b.synapse.weight\n",
    "                    bias = b.synapse.bias if b.synapse.bias is not None else torch.zeros_like(mean)\n",
    "                    W_prime = W * (gamma / torch.sqrt(var + eps)).reshape(-1, 1, 1, 1, 1)\n",
    "                    bias_prime = (bias - mean) * (gamma / torch.sqrt(var + eps)) + beta\n",
    "                    b.synapse.weight = nn.Parameter(W_prime)\n",
    "                    b.synapse.bias = nn.Parameter(bias_prime)\n",
    "                    del b.neuron.bn\n",
    "\n",
    "    def merge_pool_conv(self, b, s=2):\n",
    "        # merge pooling layer to convolutional layer\n",
    "        ori_weight = b.synapse.weight\n",
    "        ori_bias = b.synapse.bias\n",
    "        assert ori_bias is None\n",
    "        padding = b.synapse.padding[0] * s\n",
    "        stride = b.synapse.stride[0] * s\n",
    "        o,i,k1,k2,t = ori_weight.shape\n",
    "        merged_weight = torch.zeros(o, i, k1*s, k2*s, t)\n",
    "        for i in range(k1):\n",
    "            for j in range(k2):\n",
    "                merged_weight[:, :, i, j, :] = ori_weight[:,:,i//s,j//s,:]\n",
    "        self.sdnn_cnn_params['num_features'] = o\n",
    "        neuron_shape = b.neuron.shape\n",
    "        b = cuba_hoyer.Conv(self.sdnn_cnn_params, i, o, k1*s, padding=padding, stride=stride, \n",
    "                weight_scale=1, weight_norm=self.weight_norm, delay=self.delay, delay_shift=self.delay_shift)\n",
    "        b.synapse.weight = nn.Parameter(merged_weight)\n",
    "        b.neuron.shape = neuron_shape\n",
    "        return b\n",
    "    \n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format, first merge pooling layer\n",
    "        # then merge batch normalization layer\n",
    "        merge_flag = False\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            if merge_flag:\n",
    "                self.blocks[i] = self.merge_pool_conv(b)\n",
    "                merge_flag = False\n",
    "            if i < len(self.cfg) and self.cfg[i] == 'M':\n",
    "                merge_flag = True\n",
    "        self.merge_bn()\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        layer_index = 0\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            if i < len(self.cfg) and self.cfg[i] == 'M':\n",
    "                continue\n",
    "            b.export_hdf5(layer.create_group(f'{layer_index}'))\n",
    "            layer_index += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Define Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Instantiate Network, Optimizer, Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 1e-9\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VGG_Lava('VGG16', labels, dataset='CIFAR10', time_steps=1).to(device)\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# epochs = 300\n",
    "epochs = 10\n",
    "lr_reduce = 5\n",
    "steps  = [0.6,0.8,0.9]\n",
    "lr_interval = [step * epochs for step in steps]\n",
    "def lr_scale(epoch):\n",
    "    for i, step_epoch in enumerate(lr_interval):\n",
    "        if epoch < step_epoch:\n",
    "            return i\n",
    "    return i + 1\n",
    "lambda0 = lambda cur_epoch : 1.0/(lr_reduce**lr_scale(cur_epoch))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda0, last_epoch=-1)\n",
    "\n",
    "stats = slayer.utils.LearningStats()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Set log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = True\n",
    "log_file = './logs_ann/'\n",
    "\n",
    "if log:\n",
    "    try:\n",
    "        os.mkdir(log_file)\n",
    "    except OSError:\n",
    "        pass \n",
    "    identifier = 'cifar10_VGG' + '_' + datetime.datetime.now().strftime('%Y%m%d%H%M')\n",
    "    log_file+=identifier+'.log'\n",
    "    print('log file: ', log_file)\n",
    "    f = open(log_file, 'w', buffering=1)\n",
    "    f.write('\\n{}'.format(net))\n",
    "else:\n",
    "    print('use stdout')\n",
    "    f = sys.stdout\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    train_losses = AverageMeter('Loss')\n",
    "    train_event_losses = AverageMeter('Loss')\n",
    "    train_total_losses = AverageMeter('Loss')\n",
    "    train_accuracy = AverageMeter('Accuracy')\n",
    "    for i, (input, label) in enumerate(train_loader, 0):\n",
    "        input = input.to(device)\n",
    "        output,event_cost,count = net.forward(input)\n",
    "        \n",
    "        rate = output.reshape((input.shape[0], -1))\n",
    "        loss = F.cross_entropy(rate, label.to(device))\n",
    "        total_loss = loss + lam * event_cost\n",
    "        prediction = rate.data.max(1, keepdim=True)[1].cpu().flatten()\n",
    "\n",
    "        train_losses.update(loss.data.item(), input.shape[0])\n",
    "        train_event_losses.update(lam * event_cost.data.item(), input.shape[0])\n",
    "        train_total_losses.update(total_loss.data.item(), input.shape[0])\n",
    "        train_accuracy.update(torch.sum( prediction == label ).data.item() / input.shape[0], input.shape[0])\n",
    "\n",
    "        stats.training.num_samples += len(label)\n",
    "        stats.training.loss_sum += total_loss.cpu().data.item() * input.shape[0]\n",
    "        stats.training.correct_samples += torch.sum( prediction == label ).data.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        learning_rate = param_group['lr']\n",
    "    f.write(f'\\n[Epoch {epoch:2d}/{epochs}], lr: {learning_rate:.4f} loss: {train_losses.avg:.4f}, event_loss: {train_event_losses.avg:.4f}, total loss: {train_total_losses.avg:.4f}, accuracy: {train_accuracy.avg:.4f} ')\n",
    "    net.eval()\n",
    "    test_losses = AverageMeter('Loss')\n",
    "    test_event_losses = AverageMeter('Loss')\n",
    "    test_total_losses = AverageMeter('Loss')\n",
    "    test_accuracy = AverageMeter('Accuracy')\n",
    "    for i, (input, label) in enumerate(test_loader, 0):\n",
    "        # net.eval()\n",
    "        with torch.no_grad():\n",
    "            input = input.to(device)\n",
    "            output,event_cost,count = net.forward(input)\n",
    "            rate = output.reshape((input.shape[0], -1))\n",
    "\n",
    "            loss = F.cross_entropy(rate, label.to(device))\n",
    "            total_loss = loss + lam * event_cost\n",
    "            prediction = rate.data.max(1, keepdim=True)[1].cpu().flatten()\n",
    "\n",
    "            test_losses.update(loss.data.item(), input.shape[0])\n",
    "            test_event_losses.update(lam * event_cost.data.item(), input.shape[0])\n",
    "            test_total_losses.update(total_loss.data.item(), input.shape[0])\n",
    "            test_accuracy.update(torch.sum( prediction == label ).data.item() / input.shape[0], input.shape[0])\n",
    "\n",
    "        stats.testing.num_samples += len(label)\n",
    "        stats.testing.loss_sum += loss.cpu().data.item() * input.shape[0]\n",
    "        stats.testing.correct_samples += torch.sum( prediction == label ).data.item()\n",
    "    f.write(f'test: loss: {test_losses.avg:.4f}, event_loss: {test_event_losses.avg:.4f}, total loss: {test_total_losses.avg:.4f}, accuracy: {test_accuracy.avg:.4f}')\n",
    "    lr_scheduler.step()\n",
    "    if test_accuracy.avg > best_accuracy:\n",
    "        best_accuracy = test_accuracy.avg\n",
    "        new_net = copy.deepcopy(net)\n",
    "        new_net.export_hdf5(f'network_{identifier}.net')\n",
    "print('best accuracy: ', best_accuracy)\n",
    "f.write('\\nbest accuracy: {}'.format(best_accuracy))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
