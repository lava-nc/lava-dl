{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "given-quilt",
   "metadata": {},
   "source": [
    "# Strided TinyYOLOv3 sigma-delta network inference on GPU\n",
    "\n",
    "This tutorial walks through the GPU inference of trained Strided TinyYOLOv3 SDNN and describe the lava-dl slayer object detection modules along the way.\n",
    "* Refer to the [README](README.md) for the trainig script\n",
    "* Refer to [PilotNet SDNN notebook](https://github.com/lava-nc/lava-dl/blob/main/tutorials/lava/lib/dl/slayer/pilotnet/train.ipynb) for an overview of Sigma-Delta networks.\n",
    "\n",
    "## Object Detection module in Lava-DL\n",
    "Lava-dl now includes object detection module which can be accessed as `from lava.lib.dl.slayer import obd` or as `slayer.obd`. The object detection module includes\n",
    "* Base YOLO class `obd.yolo_base` which can be used to design and train YOLO SNNs.\n",
    "* Pre-formulated model descriptions and pre-trained models `obd.models.{tiny_yolov3_str, yolo_kp}`. In this tutorial we will make use of Strided TinyYOLOv3 SDNN (`obd.models.tiny_yolov3_str`).\n",
    "* Bounding box metrics and utilities `obd.bbox.{metrics, utils}` to facilitate video object detection training with spiking neurons.\n",
    "* Dataset modules and utilities `obd.dataset.{BDD, utils}`. Currently there is support for [Berkley Deep Drive (BDD100K)](https://bdd-data.berkeley.edu/) dataset. More dataset support will be added in the future.\n",
    "\n",
    "__Strided TinyYOLOv3 SDNN__: It is the Sigma-Delta version of TinyYOLOv3 object detection network designed to detect objects in a video stream. The network architecture is tweaked from the TinyYOLOv3[ref] architecture to make the best use of Loihi 2 architecture: the convolution + pool layers are replaced with strided convoltion. The model was trained using the `train_sdnn.py` script included in this folder with initial pre-training on COCO and final training on BDD100K dataset.\n",
    "```bash\n",
    "python train_sdnn [complete args]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mature-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lava.lib.dl import slayer\n",
    "from lava.lib.dl.slayer import obd\n",
    "\n",
    "import ipyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rcParams\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-willow",
   "metadata": {},
   "source": [
    "# Load Inference parameters\n",
    "\n",
    "YOLO training involves a lot of hyperparameters. The training using `train_sdnn.py` exports training hyperparameter used during the training. `slayer.utils.dotdict` can be used to easily load the hyperparameters and use the same parameters in model fine-tuning as well as inference. In this tutorial we will see `model_args.<param>` being used at various places to initialize the model, dataset, dataloader and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brown-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained_yolo_kp\n",
      "Using GPUs [0]\n",
      "\n",
      "Hyperparameters\n",
      "===============\n",
      "load : Trained_yolo_kp/network.pt\n",
      "alpha_iou : 0.8\n",
      "aug_prob : 0.4\n",
      "b : 4\n",
      "clamp_max : 5.0\n",
      "clip : 1.0\n",
      "dataset : BDD100K\n",
      "epoch : 200\n",
      "exp : None\n",
      "gpu : [0]\n",
      "label_smoothing : 0.03\n",
      "lambda_cls : 1.0\n",
      "lambda_coord : 2.0\n",
      "lambda_iou : 2.25\n",
      "lambda_noobj : 4.0\n",
      "lambda_obj : 1.8\n",
      "lr : 0.0001\n",
      "lrf : 0.01\n",
      "model : yolo_kp\n",
      "num_workers : 12\n",
      "output_dir : .\n",
      "path : data/bdd100k\n",
      "scale_grad : 0.2\n",
      "seed : None\n",
      "sp_lam : 0.01\n",
      "sp_rate : 0.01\n",
      "sparsity : True\n",
      "subset : False\n",
      "tau_grad : 0.1\n",
      "tgt_iou_thr : 0.25\n",
      "threshold : 0.1\n",
      "track_iter : 100\n",
      "verbose : False\n",
      "warmup : 40\n",
      "wd : 1e-05\n"
     ]
    }
   ],
   "source": [
    "# inference_model = 'tiny_yolov3_str'  # Loihi compatible TinyYOLOv3 network\n",
    "inference_model = 'yolo_kp'          # Customized model tragetted for 8 chip Kapoho Point form factor\n",
    "\n",
    "args = slayer.utils.dotdict(load=f'Trained_{inference_model}/network.pt')\n",
    "trained_folder = os.path.dirname(args.load)\n",
    "print(trained_folder)\n",
    "\n",
    "with open(trained_folder + '/args.txt', 'rt') as f:\n",
    "    model_args = slayer.utils.dotdict(yaml.safe_load(f))\n",
    "    for (k, v) in model_args.items():\n",
    "        if k not in args.keys():\n",
    "            args[k] = v\n",
    "            \n",
    "print('Using GPUs {}'.format(args.gpu))\n",
    "device = torch.device('cuda:{}'.format(args.gpu[0]))\n",
    "print()\n",
    "print('Hyperparameters')\n",
    "print('===============')\n",
    "for k,v in args.items():\n",
    "    print(f'{k} : {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-spring",
   "metadata": {},
   "source": [
    "# Load Network Parameters\n",
    "\n",
    "In the following cell, we will\n",
    "* Instanciate `obd.models.tiny_yolov3_str.Network` as the desired YOLO SDNN network\n",
    "* Load the pretrained model parameters into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "385e722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lecampos/lava-nx/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "if inference_model == 'tiny_yolov3_str':\n",
    "    Network = obd.models.tiny_yolov3_str.Network\n",
    "elif inference_model == 'yolo_kp':\n",
    "    Network = obd.models.yolo_kp.Network\n",
    "else:\n",
    "    raise RuntimeError\n",
    "\n",
    "net = Network(threshold=model_args.threshold,\n",
    "              tau_grad=model_args.tau_grad,\n",
    "              scale_grad=model_args.scale_grad,\n",
    "              num_classes=11,\n",
    "              clamp_max=model_args.clamp_max).to(device)\n",
    "net.init_model((448, 448))\n",
    "# net.load_state_dict(torch.load(args.load))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a55916-5a1d-47ca-b268-1997db4494e9",
   "metadata": {},
   "source": [
    "# Create YOLO target\n",
    "\n",
    "YOLO training requires target tensor generation. Lava-DL object detction module provides `obd.YOLOtarget` to generate the YOLO target for the network on the fly. `obd.YOLOtarget` also includes `collate_fn` method which can be used to stack the samples, targets and ground truth bounding boxes in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d883bad9-c061-498d-a76f-a2f4c7469c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_target = obd.YOLOtarget(anchors=net.anchors,\n",
    "                             scales=net.scale,\n",
    "                             num_classes=net.num_classes,\n",
    "                             ignore_iou_thres=model_args.tgt_iou_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3adf1",
   "metadata": {},
   "source": [
    "# Load Dataset and Dataloader\n",
    "\n",
    "Here we will make use of `obd.dataset.BDD` to create the required dataset for BDD100K inference. The dataloader is the standard PyTorch dataloader instance. Note the use of `yolo_target.collate_fn` to generate traget YOLO tensors and describe the batch samples stacking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0e496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = obd.dataset.BDD(root=args.path, dataset='track', train=False, randomize_seq=False, seq_len=200)\n",
    "\n",
    "test_loader = DataLoader(test_set,\n",
    "                         batch_size=1,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=yolo_target.collate_fn,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562ca5d",
   "metadata": {},
   "source": [
    "# Run GPU Inference\n",
    "\n",
    "GPU inferene of the network follows the standard PyTorch testing iteration through the test_set once. We make use of `obd.bbox.metrics.APstats` module to evaluate mean Averge Precision (mAP) performance of the YOLO SDNN network.\n",
    "\n",
    "Note the redifinition of accuracy string in `slayer.utils.LearningStats` and it's use in printing the progress interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f307ea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs  torch.Size([1, 3, 448, 448, 200])\n",
      "predictions  200 torch.Size([300, 6]) torch.Size([300, 512])\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "stats = slayer.utils.LearningStats(accuracy_str='AP@0.5')\n",
    "t_st = datetime.now()\n",
    "ap_stats = obd.bbox.metrics.APstats(iou_threshold=0.5)\n",
    "\n",
    "for i, (inputs, targets, bboxes) in enumerate(test_loader):\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('inputs ', inputs.shape)\n",
    "        inputs = inputs.to(device)\n",
    "        predictions, counts, latent_space = net(inputs, latent_space_backcounter=1)\n",
    "        T = inputs.shape[-1]\n",
    "        predictions = [obd.bbox.utils.nms_ls(predictions[..., t], latent_space[..., t]) for t in range(T)]\n",
    "        print(\"predictions \", len(predictions), predictions[0][0][0].shape, predictions[0][0][1].shape)\n",
    "\n",
    "        break\n",
    "\n",
    "        for t in range(T):\n",
    "            ap_stats.update(predictions[t], bboxes[t])\n",
    "\n",
    "        stats.testing.num_samples += inputs.shape[0]\n",
    "        stats.testing.correct_samples = ap_stats[:] * stats.testing.num_samples\n",
    "\n",
    "        processed = i * test_loader.batch_size\n",
    "        total = len(test_loader.dataset)\n",
    "        time_elapsed = (datetime.now() - t_st).total_seconds()\n",
    "        samples_sec = time_elapsed / (i + 1) / test_loader.batch_size\n",
    "        header_list = [f'Test: [{processed}/{total} ({100.0 * processed / total:.0f}%)]']\n",
    "\n",
    "        stats_str = f'| {header_list[0]} AP{str(stats).split(\"AP\")[1]}'\n",
    "        print(f'\\rIter:{i}, {samples_sec:.3f}samples/sec {stats_str}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386b5ff",
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "\n",
    "The lava-dl object detection module also provides visualization utilities which can be used to create video frames and animations of the network output.\n",
    "* `obd.bbox.utils.mark_bounding_boxes` to mark bounding box to image.\n",
    "* `obd.bbox.utils.create_frames` to create annotated image frames for a sequence of predictions and ground truth.\n",
    "* `obd.bbox.utils.create_video` to export the annotated video for a sequence of predictions and ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec430f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize a few frames\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# obd.bbox.utils.create_video(inputs, bboxes, predictions, 'yolo_output', test_set.classes)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mobd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      5\u001b[0m img_plt \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mimshow(frames[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/leo-internal/lava-dl/src/lava/lib/dl/slayer/object_detection/boundingbox/utils.py:705\u001b[0m, in \u001b[0;36mcreate_frames\u001b[0;34m(inputs, targets, predictions, classes, batch, box_color_map)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    699\u001b[0m     image \u001b[38;5;241m=\u001b[39m Img\u001b[38;5;241m.\u001b[39mfromarray(\n\u001b[1;32m    700\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8(\n\u001b[1;32m    701\u001b[0m             inputs[b, :, :, :, t]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    702\u001b[0m             \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m    703\u001b[0m         )\n\u001b[1;32m    704\u001b[0m     )\n\u001b[0;32m--> 705\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m \u001b[43mannotation_from_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mconfidence_th\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m     marked_img \u001b[38;5;241m=\u001b[39m mark_bounding_boxes(image,\n\u001b[1;32m    711\u001b[0m                                      annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    712\u001b[0m                                      box_color_map\u001b[38;5;241m=\u001b[39mbox_color_map,\n\u001b[1;32m    713\u001b[0m                                      thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    714\u001b[0m     draw \u001b[38;5;241m=\u001b[39m ImageDraw\u001b[38;5;241m.\u001b[39mDraw(marked_img)\n",
      "File \u001b[0;32m~/leo-internal/lava-dl/src/lava/lib/dl/slayer/object_detection/boundingbox/utils.py:244\u001b[0m, in \u001b[0;36mannotation_from_tensor\u001b[0;34m(tensor, frame_size, object_names, confidence_th, normalized)\u001b[0m\n\u001b[1;32m    240\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    242\u001b[0m objects \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 244\u001b[0m boxes \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    245\u001b[0m xmin \u001b[38;5;241m=\u001b[39m (boxes[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m boxes[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m width\n\u001b[1;32m    246\u001b[0m ymin \u001b[38;5;241m=\u001b[39m (boxes[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m boxes[:, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m height\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Visualize a few frames\n",
    "# obd.bbox.utils.create_video(inputs, bboxes, predictions, 'yolo_output', test_set.classes)\n",
    "frames = obd.bbox.utils.create_frames(inputs, bboxes, predictions, test_set.classes)\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "img_plt = ax.imshow(frames[0])\n",
    "ax.axis('off')\n",
    "\n",
    "rcParams['animation.embed_limit'] = 1<<20\n",
    "anim = animation.FuncAnimation(fig, lambda i: img_plt.set_data(frames[i]),\n",
    "                               frames=len(frames), interval=40, repeat=True)\n",
    "plt.close(fig)\n",
    "\n",
    "num_frames = 5\n",
    "fig, ax = plt.subplots(num_frames, 1, figsize=(8, 4 * num_frames))\n",
    "for i in range(num_frames):\n",
    "    ax[i].imshow(frames[i])\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(f'Frame {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfb956-aab9-4d3f-8a86-0c8c92f89536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"yolo_output.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize video\n",
    "anim.save('yolo_output.mp4', writer=animation.FFMpegWriter())\n",
    "ipd.Video('yolo_output.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
