{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "given-quilt",
   "metadata": {},
   "source": [
    "# Strided TinyYOLOv3 sigma-delta network inference on GPU\n",
    "\n",
    "This tutorial walks through the GPU inference of trained Strided TinyYOLOv3 SDNN and describe the lava-dl slayer object detection modules along the way.\n",
    "* Refer to the [README](README.md) for the trainig script\n",
    "* Refer to [PilotNet SDNN notebook](https://github.com/lava-nc/lava-dl/blob/main/tutorials/lava/lib/dl/slayer/pilotnet/train.ipynb) for an overview of Sigma-Delta networks.\n",
    "\n",
    "## Object Detection module in Lava-DL\n",
    "Lava-dl now includes object detection module which can be accessed as `from lava.lib.dl.slayer import obd` or as `slayer.obd`. The object detection module includes\n",
    "* Base YOLO class `obd.yolo_base` which can be used to design and train YOLO SNNs.\n",
    "* Pre-formulated model descriptions and pre-trained models `obd.models.{tiny_yolov3_str, yolo_kp}`. In this tutorial we will make use of Strided TinyYOLOv3 SDNN (`obd.models.tiny_yolov3_str`).\n",
    "* Bounding box metrics and utilities `obd.bbox.{metrics, utils}` to facilitate video object detection training with spiking neurons.\n",
    "* Dataset modules and utilities `obd.dataset.{BDD, utils}`. Currently there is support for [Berkley Deep Drive (BDD100K)](https://bdd-data.berkeley.edu/) dataset. More dataset support will be added in the future.\n",
    "\n",
    "__Strided TinyYOLOv3 SDNN__: It is the Sigma-Delta version of TinyYOLOv3 object detection network designed to detect objects in a video stream. The network architecture is tweaked from the TinyYOLOv3[ref] architecture to make the best use of Loihi 2 architecture: the convolution + pool layers are replaced with strided convoltion. The model was trained using the `train_sdnn.py` script included in this folder with initial pre-training on COCO and final training on BDD100K dataset.\n",
    "```bash\n",
    "python train_sdnn [complete args]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lava.lib.dl import slayer\n",
    "from lava.lib.dl.slayer import obd\n",
    "\n",
    "import ipyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rcParams\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-willow",
   "metadata": {},
   "source": [
    "# Load Inference parameters\n",
    "\n",
    "YOLO training involves a lot of hyperparameters. The training using `train_sdnn.py` exports training hyperparameter used during the training. `slayer.utils.dotdict` can be used to easily load the hyperparameters and use the same parameters in model fine-tuning as well as inference. In this tutorial we will see `model_args.<param>` being used at various places to initialize the model, dataset, dataloader and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_model = 'tiny_yolov3_str'  # Loihi compatible TinyYOLOv3 network\n",
    "inference_model = 'yolo_kp'          # Customized model tragetted for 8 chip Kapoho Point form factor\n",
    "\n",
    "args = slayer.utils.dotdict(load=f'Trained_{inference_model}/network.pt')\n",
    "trained_folder = os.path.dirname(args.load)\n",
    "print(trained_folder)\n",
    "\n",
    "with open(trained_folder + '/args.txt', 'rt') as f:\n",
    "    model_args = slayer.utils.dotdict(yaml.safe_load(f))\n",
    "    for (k, v) in model_args.items():\n",
    "        if k not in args.keys():\n",
    "            args[k] = v\n",
    "            \n",
    "print('Using GPUs {}'.format(args.gpu))\n",
    "device = torch.device('cuda:{}'.format(args.gpu[0]))\n",
    "print()\n",
    "print('Hyperparameters')\n",
    "print('===============')\n",
    "for k,v in args.items():\n",
    "    print(f'{k} : {v}')\n",
    "    \n",
    "args.b = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-spring",
   "metadata": {},
   "source": [
    "# Load Network Parameters\n",
    "\n",
    "In the following cell, we will\n",
    "* Instanciate `obd.models.tiny_yolov3_str.Network` as the desired YOLO SDNN network\n",
    "* Load the pretrained model parameters into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference_model == 'tiny_yolov3_str':\n",
    "    Network = obd.models.tiny_yolov3_str.Network\n",
    "elif inference_model == 'yolo_kp':\n",
    "    Network = obd.models.yolo_kp.Network\n",
    "else:\n",
    "    raise RuntimeError\n",
    "\n",
    "net = Network(threshold=model_args.threshold,\n",
    "              tau_grad=model_args.tau_grad,\n",
    "              scale_grad=model_args.scale_grad,\n",
    "              num_classes=11,\n",
    "              clamp_max=model_args.clamp_max).to(device)\n",
    "net.init_model((448, 448))\n",
    "# net.load_state_dict(torch.load(args.load))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a55916-5a1d-47ca-b268-1997db4494e9",
   "metadata": {},
   "source": [
    "# Create YOLO target\n",
    "\n",
    "YOLO training requires target tensor generation. Lava-DL object detction module provides `obd.YOLOtarget` to generate the YOLO target for the network on the fly. `obd.YOLOtarget` also includes `collate_fn` method which can be used to stack the samples, targets and ground truth bounding boxes in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883bad9-c061-498d-a76f-a2f4c7469c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_target = obd.YOLOtarget(anchors=net.anchors,\n",
    "                             scales=net.scale,\n",
    "                             num_classes=net.num_classes,\n",
    "                             ignore_iou_thres=model_args.tgt_iou_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3adf1",
   "metadata": {},
   "source": [
    "# Load Dataset and Dataloader\n",
    "\n",
    "Here we will make use of `obd.dataset.BDD` to create the required dataset for BDD100K inference. The dataloader is the standard PyTorch dataloader instance. Note the use of `yolo_target.collate_fn` to generate traget YOLO tensors and describe the batch samples stacking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = obd.dataset.BDD(root=args.path, dataset='track', train=False, randomize_seq=False, seq_len=200)\n",
    "\n",
    "test_loader = DataLoader(test_set,\n",
    "                         batch_size=args.b,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=yolo_target.collate_fn,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562ca5d",
   "metadata": {},
   "source": [
    "# Run GPU Inference\n",
    "\n",
    "GPU inferene of the network follows the standard PyTorch testing iteration through the test_set once. We make use of `obd.bbox.metrics.APstats` module to evaluate mean Averge Precision (mAP) performance of the YOLO SDNN network.\n",
    "\n",
    "Note the redifinition of accuracy string in `slayer.utils.LearningStats` and it's use in printing the progress interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "stats = slayer.utils.LearningStats(accuracy_str='AP@0.5')\n",
    "t_st = datetime.now()\n",
    "ap_stats = obd.bbox.metrics.APstats(iou_threshold=0.5)\n",
    "\n",
    "for i, (inputs, targets, bboxes) in enumerate(test_loader):\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('inputs ', inputs.shape)\n",
    "        inputs = inputs.to(device)\n",
    "        predictions, counts, latent_space = net(inputs, latent_space_backcounter=1)\n",
    "        T = inputs.shape[-1]\n",
    "        predictions = [obd.bbox.utils.nms_ls(predictions[..., t], latent_space[..., t]) for t in range(T)]\n",
    "        # print(\"predictions \", len(predictions), predictions[0][0][0].shape, predictions[0][0][1].shape)\n",
    "        print(\"predictions time step\", len(predictions))\n",
    "        for t in range(T):\n",
    "            for batch in range(args.b):\n",
    "                for idx in  range(predictions[t][batch]['detections'].shape[0]):\n",
    "                    print(\"detections \", predictions[t][batch]['detections'][idx])\n",
    "                    print(\"detections latent \", predictions[t][batch]['latent'][idx])\n",
    "            \n",
    "        \n",
    "        # print(\"detections: \", predictions[0]['detections'])\n",
    "        \n",
    "\n",
    "        break\n",
    "\n",
    "        for t in range(T):\n",
    "            ap_stats.update(predictions[t], bboxes[t])\n",
    "\n",
    "        stats.testing.num_samples += inputs.shape[0]\n",
    "        stats.testing.correct_samples = ap_stats[:] * stats.testing.num_samples\n",
    "\n",
    "        processed = i * test_loader.batch_size\n",
    "        total = len(test_loader.dataset)\n",
    "        time_elapsed = (datetime.now() - t_st).total_seconds()\n",
    "        samples_sec = time_elapsed / (i + 1) / test_loader.batch_size\n",
    "        header_list = [f'Test: [{processed}/{total} ({100.0 * processed / total:.0f}%)]']\n",
    "\n",
    "        stats_str = f'| {header_list[0]} AP{str(stats).split(\"AP\")[1]}'\n",
    "        print(f'\\rIter:{i}, {samples_sec:.3f}samples/sec {stats_str}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386b5ff",
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "\n",
    "The lava-dl object detection module also provides visualization utilities which can be used to create video frames and animations of the network output.\n",
    "* `obd.bbox.utils.mark_bounding_boxes` to mark bounding box to image.\n",
    "* `obd.bbox.utils.create_frames` to create annotated image frames for a sequence of predictions and ground truth.\n",
    "* `obd.bbox.utils.create_video` to export the annotated video for a sequence of predictions and ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec430f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize a few frames\n",
    "# # obd.bbox.utils.create_video(inputs, bboxes, predictions, 'yolo_output', test_set.classes)\n",
    "# frames = obd.bbox.utils.create_frames(inputs, bboxes, predictions, test_set.classes)\n",
    "# fig, ax = plt.subplots(figsize=(6, 3))\n",
    "# img_plt = ax.imshow(frames[0])\n",
    "# ax.axis('off')\n",
    "\n",
    "# rcParams['animation.embed_limit'] = 1<<20\n",
    "# anim = animation.FuncAnimation(fig, lambda i: img_plt.set_data(frames[i]),\n",
    "#                                frames=len(frames), interval=40, repeat=True)\n",
    "# plt.close(fig)\n",
    "\n",
    "# num_frames = 5\n",
    "# fig, ax = plt.subplots(num_frames, 1, figsize=(8, 4 * num_frames))\n",
    "# for i in range(num_frames):\n",
    "#     ax[i].imshow(frames[i])\n",
    "#     ax[i].axis('off')\n",
    "#     ax[i].set_title(f'Frame {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfb956-aab9-4d3f-8a86-0c8c92f89536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize video\n",
    "# anim.save('yolo_output.mp4', writer=animation.FFMpegWriter())\n",
    "# ipd.Video('yolo_output.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
