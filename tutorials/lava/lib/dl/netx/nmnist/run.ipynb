{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e2575c",
   "metadata": {},
   "source": [
    "# NMNIST NetX Tutorial\n",
    "Hello everyone, in this tutorial of lava-dl, we are going to convert the model trained in the Slayer NMNIST tutorial to lava processes and run it on Loihi simulator.\n",
    "\n",
    "Network excange module is available as `lava.lib.dl.netx.{hdf5, blocks, utils}`.\n",
    "* `hdf5` implements automatic network generation.\n",
    "* `blocks` implements individual layer blocks.\n",
    "* `utils` implements hdf5 reading utilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42828fba",
   "metadata": {},
   "source": [
    "## Import Phase\n",
    "First, let us import the libraries needed for this tutorial. If you want to investigate why they are for, you can right-click the module and go to the declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69357c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Import io module\n",
    "from lava.proc import io\n",
    "\n",
    "# Import Process level primitives\n",
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.process.ports.ports import InPort, OutPort\n",
    "# Import parent classes for ProcessModels\n",
    "from lava.magma.core.model.py.model import PyLoihiProcessModel\n",
    "# Import ProcessModel ports, data-types\n",
    "from lava.magma.core.model.py.ports import PyInPort, PyOutPort\n",
    "from lava.magma.core.model.py.type import LavaPyType\n",
    "# Import execution protocol and hardware resources\n",
    "from lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n",
    "\n",
    "from lava.magma.core.resources import GPU #You can change it to the CPU if you wish.\n",
    "# Import decorators\n",
    "from lava.magma.core.decorator import implements, requires\n",
    "# Import Compile and run modules\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg\n",
    "from lava.magma.core.run_conditions import RunSteps\n",
    "\n",
    "#Import network exchange module  \n",
    "from lava.lib.dl import netx\n",
    "# Import Dataset  \n",
    "from nmnist import NMNISTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db382d",
   "metadata": {},
   "source": [
    "## Network Initiation\n",
    "Now, we will import the trained hdf5 file from the 'Trained' folder and print it before and after the exchange happens. In the tutorial folder, it is already included. If you wish, you can replace that one with the one you have trained with the Slayer NMNIST tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b416e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Type   |  W  |  H  |  C  | ker | str | pad | dil | grp |delay|\n",
      "|Dense     |    1|    1|  512|     |     |     |     |     |True |\n",
      "|Dense     |    1|    1|  512|     |     |     |     |     |True |\n",
      "|Dense     |    1|    1|   10|     |     |     |     |     |False|\n",
      "There are 3 layers in network:\n",
      "Dense: Process_1 , shape : (512,)\n",
      "Dense: Process_4 , shape : (512,)\n",
      "Dense: Process_7 , shape : (10,)\n"
     ]
    }
   ],
   "source": [
    "net = netx.hdf5.Network(net_config='Trained/network.net')\n",
    "print(net)\n",
    "\n",
    "print(f'There are {len(net)} layers in network:')\n",
    "for l in net.layers:\n",
    "    print(f'{l.block:5s}: {l.name:10s}, shape : {l.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a799e23",
   "metadata": {},
   "source": [
    "## Dataset Instance Creation:\n",
    "Here we initiate our dataset instance. But this NMNIST code is a modified version of the original ( [Reference](https://www.frontiersin.org/articles/10.3389/fnins.2015.00437/full)  ) NMNIST dataset code. However, credentials are conserved. If you experience a download issue, you can copy and paste the dataset available in the Slayer NMNIST training tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f827c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = NMNISTDataset(\n",
    "    path='data', \n",
    "    train=False, # This mark will give us the test set of the Dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ead1a6",
   "metadata": {},
   "source": [
    "Now we will specify some of our variables and choose random samples from the dataset instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3adc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 25 # You can change this value for your run.\n",
    "\n",
    "num_steps_per_image = 300 #This value is adjusted for the NMNIST dataset. 300ms of events will be supplied.\n",
    "\n",
    "num_steps = num_images * num_steps_per_image\n",
    "\n",
    "idx = np.random.choice(np.arange(len(data_set.samples)),num_images,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3607b1",
   "metadata": {},
   "source": [
    "## Process Definitions\n",
    "Our system will consist of 3 main parts. \n",
    "### First Part: Spike Input\n",
    "The first part will be the part in which Spike Inputs are supplied to the network.\n",
    "### Second Part: Classifier Network\n",
    "We have already created this part by converting our model by using lava.dl.NetX\n",
    "### Third Part: Output\n",
    "In this last part, our output process will accumulate the outputs of the network and give us which prediction is made. Also, ground truth data also will be supplied to this process by Spike Input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9afdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpikeInput(AbstractProcess):\n",
    "    \"\"\"Reads image data from the NMNIST dataset \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        n_img = kwargs.pop('num_images', 25)\n",
    "        n_steps_img = kwargs.pop('num_steps_per_image', 300)\n",
    "        shape = (2312,)\n",
    "        self.spikes_out = OutPort(shape=shape)  # Input spikes to the classifier\n",
    "        self.label_out = OutPort(shape=(1,))  # Ground truth labels to OutputProc\n",
    "        self.num_images = Var(shape=(1,), init=n_img)\n",
    "        self.num_steps_per_image = Var(shape=(1,), init=n_steps_img)\n",
    "        self.input_img = Var(shape=shape)\n",
    "        self.ground_truth_label = Var(shape=(1,))\n",
    "        self.v = Var(shape=shape, init=0)\n",
    "        self.vth = Var(shape=(1,), init=kwargs['vth'])\n",
    "\n",
    "class OutputProcess(AbstractProcess):\n",
    "    \"\"\"Process to gather spikes from 10 output LIF neurons and interpret the\n",
    "    highest spiking rate as the classifier output\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        shape = (10,)\n",
    "        n_img = kwargs.pop('num_images', 25)\n",
    "        n_steps_img  = kwargs.pop('num_steps_per_image', 300)\n",
    "        self.num_images = Var(shape=(1,), init=n_img)\n",
    "        self.spikes_in = InPort(shape=shape)\n",
    "        self.label_in = InPort(shape=(1,))\n",
    "        self.spikes_accum = Var(shape=shape)  # Accumulated spikes for classification\n",
    "        self.num_steps_per_image = Var(shape=(1,), init=n_steps_img )\n",
    "        self.pred_labels = Var(shape=(n_img,))\n",
    "        self.gt_labels = Var(shape=(n_img,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69154951",
   "metadata": {},
   "source": [
    "## Process Model Definitions\n",
    "Now we will define Process models for the processes we have defined previously.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bc1e9",
   "metadata": {},
   "source": [
    "### Input Process Model\n",
    "This model will run in the Input process to supply the network with the data. The important part is that this model will supply the next event instance of the same number in every run step and will change to a different number every 300 steps. Decorators are used to specifying which protocol the process should use and which resource should be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147c9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "@implements(proc=SpikeInput, protocol=LoihiProtocol)\n",
    "@requires(GPU) #You can change it to the CPU if you wish.\n",
    "class PySpikeInputModel(PyLoihiProcessModel):\n",
    "    num_images: int = LavaPyType(int, int, precision=32)\n",
    "    spikes_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, bool, precision=1)\n",
    "    label_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, np.int32,\n",
    "                                      precision=32)\n",
    "    num_steps_per_image: int = LavaPyType(int, int, precision=32)\n",
    "    input_img: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n",
    "    ground_truth_label: int = LavaPyType(int, int, precision=32)\n",
    "    v: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n",
    "    vth: int = LavaPyType(int, int, precision=32)\n",
    "    \n",
    "    def __init__(self, proc_params):\n",
    "        super().__init__(proc_params=proc_params)\n",
    "        self.nmnist_dataset = data_set\n",
    "        self.curr_img_id = 0\n",
    "  \n",
    "        self.idx = idx\n",
    "    def post_guard(self):\n",
    "        \n",
    "        if self.time_step % self.num_steps_per_image == 0:\n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            img ,self.ground_truth_label = self.nmnist_dataset[self.idx[self.curr_img_id]]\n",
    "\n",
    "\n",
    "            self.input_img = img[:, self.time_step % self.num_steps_per_image].astype(np.int32)\n",
    "\n",
    "        return False\n",
    "\n",
    "    def run_post_mgmt(self):\n",
    "        \"\"\"Post-Management phase: executed only when guard function above \n",
    "        returns True.\n",
    "        \"\"\"\n",
    "        img ,self.ground_truth_label = self.nmnist_dataset[self.idx[self.curr_img_id]]\n",
    "\n",
    "        self.input_img = img[:,0].astype(np.int32) \n",
    "        self.v = np.zeros(self.v.shape)\n",
    "        self.label_out.send(np.array([self.ground_truth_label]))\n",
    "        self.curr_img_id += 1\n",
    "\n",
    "    def run_spk(self):\n",
    "        \"\"\"Spiking phase: executed unconditionally at every time-step\n",
    "        \"\"\"\n",
    "        self.v[:] = self.v + self.input_img\n",
    "        s_out = self.v > self.vth\n",
    "        self.v[s_out] = 0  # reset voltage to 0 after a spike\n",
    "        self.spikes_out.send(s_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df117c38",
   "metadata": {},
   "source": [
    "### Output Process Model\n",
    "This model will run in the Output Process. Here this model makes the process to detect the most fired neuron of the output layer, so the prediction is obtained. Again, decorators are used to specifying which protocol the process should use and which resource should be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b15484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@implements(proc=OutputProcess, protocol=LoihiProtocol)\n",
    "@requires(GPU) #You can change it to the CPU if you wish.\n",
    "class PyOutputProcessModel(PyLoihiProcessModel):\n",
    "    label_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, int, precision=32)\n",
    "    spikes_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, bool, precision=1)\n",
    "    num_images: int = LavaPyType(int, int, precision=32)\n",
    "    spikes_accum: np.ndarray = LavaPyType(np.ndarray, np.int32, precision=32)\n",
    "    num_steps_per_image: int = LavaPyType(int, int, precision=32)\n",
    "    pred_labels: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n",
    "    gt_labels: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n",
    "        \n",
    "    def __init__(self, proc_params):\n",
    "        super().__init__(proc_params=proc_params)\n",
    "        self.current_img_id = 0\n",
    "\n",
    "    def post_guard(self):\n",
    "        \"\"\"Guard function for PostManagement phase.\n",
    "        \"\"\"\n",
    "        if self.time_step % self.num_steps_per_image == 0 and \\\n",
    "                self.time_step > 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def run_post_mgmt(self):\n",
    "        \"\"\"Post-Management phase: executed only when guard function above \n",
    "        returns True.\n",
    "        \"\"\"\n",
    "        gt_label = self.label_in.recv()\n",
    "        pred_label = np.argmax(self.spikes_accum)\n",
    "        self.gt_labels[self.current_img_id] = gt_label\n",
    "        self.pred_labels[self.current_img_id] = pred_label\n",
    "        self.current_img_id += 1\n",
    "        self.spikes_accum = np.zeros_like(self.spikes_accum)\n",
    "\n",
    "    def run_spk(self):\n",
    "        \"\"\"Spiking phase: executed unconditionally at every time-step\n",
    "        \"\"\"\n",
    "        spk_in = self.spikes_in.recv()\n",
    "        self.spikes_accum = self.spikes_accum + spk_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa40a6",
   "metadata": {},
   "source": [
    "### Process Creation\n",
    "Here we are creating the objects of the processes we have just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947e15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_input = SpikeInput(num_images=num_images,\n",
    "                         num_steps_per_image=num_steps_per_image,\n",
    "                         vth=1)\n",
    "output_proc = OutputProcess(num_images=num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3d8a6",
   "metadata": {},
   "source": [
    "## Connection of Processes\n",
    "As we have defined and created our processes, we will connect them to each other to be functionalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc72140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect Processes\n",
    "spike_input.spikes_out.connect(net.in_layer.synapse.s_in) # Spike Input layer to Network\n",
    "spike_input.label_out.connect(output_proc.label_in) # Spike Input layer to Output layer for the ground truth\n",
    "net.out_layer.out.connect(output_proc.spikes_in) # Network output to the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148afcff",
   "metadata": {},
   "source": [
    "## Run Phase\n",
    "Now we have come to the sweetest part of our tutorial, running and showing the results. Here in the for loop number of samples are processed with the steps specified. Then the results are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cade696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image: 25\n",
      "Ground truth: [3 5 1 0 5 4 8 4 4 6 7 6 4 3 7 2 6 1 9 7 5 6 6 8 4]\n",
      "Predictions : [3 5 1 0 5 4 8 4 4 6 7 6 4 3 7 2 6 1 9 7 5 6 6 8 4]\n",
      "Accuracy    : 100.0\n"
     ]
    }
   ],
   "source": [
    "for img_id in range(num_images):\n",
    "    print(f\"\\rCurrent image: {img_id+1}\", end=\"\")\n",
    "    # Run each event-inference for fixed number of steps\n",
    "    net.run(\n",
    "        condition=RunSteps(num_steps=num_steps_per_image,),\n",
    "        run_cfg=Loihi1SimCfg(select_sub_proc_model=True,\n",
    "                             select_tag='fixed_pt'))\n",
    "    \n",
    "\n",
    "ground_truth = output_proc.gt_labels.get().astype(np.int32)\n",
    "predictions = output_proc.pred_labels.get().astype(np.int32)\n",
    "\n",
    "net.stop()\n",
    "accuracy = np.sum(ground_truth==predictions)/ground_truth.size * 100\n",
    "\n",
    "print(f\"\\nGround truth: {ground_truth}\\n\"\n",
    "      f\"Predictions : {predictions}\\n\"\n",
    "      f\"Accuracy    : {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec53ce",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This was the end of our tutorial. Trusting your accuracy is high, I hope you have enjoyed it.\n",
    "### Credit\n",
    "This tutorial is created by Ahmet Akman, undergraduate student at [Middle East Technical University](metu.edu.tr), [Electrical ,and Electronics Engineering Department](eee.metu.edu.tr) , and undergraduate researcher at [METU Center for Image Analysis](http://ogam.metu.edu.tr/en/) For further information follow [github](github.com/ahmetakman) , [resume](dar.vin/ahmetakman_resume) and [linkedin](https://www.linkedin.com/in/ahmet-akman-039b05148/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
